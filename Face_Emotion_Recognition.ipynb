{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face Emotion Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+GfCS0YRYhCtmEXpMIeNt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anku0210/Face-emotion-Recognition/blob/main/Face_Emotion_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Name: Live Class Monitoring System(Face Emotion Recognition)\n",
        "\n"
      ],
      "metadata": {
        "id": "S9u3poRy4ttG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Indroduction \n",
        "\n",
        "## The Indian education landscape has been undergoing rapid changes for the past 10 years owing to the advancement of web-based learning services, specifically, eLearning platforms.\n",
        "\n",
        "## Global E-learning is estimated to witness an 8X over the next 5 years to reach USD 2B in 2021. India is expected to grow with a CAGR of 44% crossing the 10M users mark in 2021. Although the market is growing on a rapid scale, there are major challenges associated with digital learning when compared with brick and mortar classrooms.\n",
        "\n",
        "## One of many challenges is how to ensure quality learning for students. Digital platforms might overpower physical classrooms in terms of content quality but when it comes to understanding whether students are able to grasp the content in a live class scenario is yet an open-end challenge.\n",
        "\n",
        "## In a physical classroom during a lecturing teacher can see the faces and assess the emotion of the class and tune their lecture accordingly, whether he is going fast or slow. He can identify students who need special attention.\n",
        "\n",
        "## Digital classrooms are conducted via video telephony software program (exZoom) where it’s not possible for medium scale class (25-50) to see all students and access the mood. Because of this drawback, students are not focusing on content due to lack of surveillance.\n",
        "\n",
        "## While digital platforms have limitations in terms of physical surveillance but it comes with the power of data and machines which can work for you. It provides data in the form of video, audio, and texts which can be analysed using deep learning algorithms.\n",
        "\n",
        "## Deep learning backed system not only solves the surveillance issue, but it also removes the human bias from the system, and all information is no longer in the teacher’s brain rather translated in numbers that can be analysed and tracked.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yBQtO_Bt47pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement\n",
        "\n",
        "## We will solve the above-mentioned challenge by applying deep learning algorithms to live video data.\n",
        "\n",
        "## The solution to this problem is by recognizing facial emotions."
      ],
      "metadata": {
        "id": "YVp-gklx9jIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Face Emotion Recognition ?\n",
        "\n",
        "## Facial expression recognition software is a technology which uses biometric markers to detect emotions in human faces. More precisely, this technology is a sentiment analysis tool and is able to automatically detect the six basic or universal expressions: happiness, sadness, anger, surprise, fear, and disgust."
      ],
      "metadata": {
        "id": "noTq-4m29z5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "gJEPxtkm-Un2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPool2D, Flatten,Dense,Dropout,BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import cv2\n",
        "from tensorflow.keras.applications import VGG16, InceptionResNetV2\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam,RMSprop,SGD,Adamax"
      ],
      "metadata": {
        "id": "mZHdjgww9wBs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YhTISPOi4oV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9721cc29-e956-4ca6-e14f-3fb4415ad724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/si11cws2pyho1bp/archive.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_gt6fbYtLQU",
        "outputId": "e0e3cf38-6211-44e5-821b-58ac2ddea19d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-25 10:08:25--  https://www.dropbox.com/s/si11cws2pyho1bp/archive.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.9.18, 2620:100:601b:18::a27d:812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.9.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/si11cws2pyho1bp/archive.zip [following]\n",
            "--2022-05-25 10:08:26--  https://www.dropbox.com/s/raw/si11cws2pyho1bp/archive.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uce0ea463e7d3a8440bf8e57b90c.dl.dropboxusercontent.com/cd/0/inline/Bl64sg2z7O241waJv6PZyOxOPy-qCAm_soUr43_bzIf3KATwKuQ9PXNcHAhf8ePfJ2D0J09JzivdpjoXJ0dFsyCnCXcMFoWeXWXUdSOmdNIR55_fFTumvXjinfy3frH5qoRuqkkBhMUMzn2wXGDQEIMyGs-fmGEZyGECCpRqIR4lkQ/file# [following]\n",
            "--2022-05-25 10:08:26--  https://uce0ea463e7d3a8440bf8e57b90c.dl.dropboxusercontent.com/cd/0/inline/Bl64sg2z7O241waJv6PZyOxOPy-qCAm_soUr43_bzIf3KATwKuQ9PXNcHAhf8ePfJ2D0J09JzivdpjoXJ0dFsyCnCXcMFoWeXWXUdSOmdNIR55_fFTumvXjinfy3frH5qoRuqkkBhMUMzn2wXGDQEIMyGs-fmGEZyGECCpRqIR4lkQ/file\n",
            "Resolving uce0ea463e7d3a8440bf8e57b90c.dl.dropboxusercontent.com (uce0ea463e7d3a8440bf8e57b90c.dl.dropboxusercontent.com)... 162.125.9.15, 2620:100:601b:15::a27d:80f\n",
            "Connecting to uce0ea463e7d3a8440bf8e57b90c.dl.dropboxusercontent.com (uce0ea463e7d3a8440bf8e57b90c.dl.dropboxusercontent.com)|162.125.9.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Bl5SNh-JDMyzPM-4AkEk2cxneyEosmYfqRG5iOlBcvl_rCxoMhwa4Pe_IwIWXAR1bF9WpXLF4dsD6NBK2QyX5uf84Qim-gq-59PQXcBJOcc_wNB4Ak5jFZMVWCq3qtJbKNfD7BmCGZ2u6qg_41r7hRA4oxQPRCBOE9Fxq4_-vtQKZy_YFEXuw0aYnvkdv9zoQn9iZhCw8-aY8hv9BBasTprvICAefJaQFGZ-DpOgeUf62yvDsR20k4mKHKaBZSSwKLFQ2QlW0JItw_BpCWOQAXAdeg222YFNKzkeB2pz-tAewy5X_SsOX603VbowTbVSRb0-BjYBfBsi81IhWjS9fs_DTRe8uENhb2g0X8gd0pZQYJeuP2SBkqW7OmHjQbVoJ7I2rERbyQ1OUxRejfL9hzKR_lDdxT7xUztmYmAPgVef6g/file [following]\n",
            "--2022-05-25 10:08:26--  https://uce0ea463e7d3a8440bf8e57b90c.dl.dropboxusercontent.com/cd/0/inline2/Bl5SNh-JDMyzPM-4AkEk2cxneyEosmYfqRG5iOlBcvl_rCxoMhwa4Pe_IwIWXAR1bF9WpXLF4dsD6NBK2QyX5uf84Qim-gq-59PQXcBJOcc_wNB4Ak5jFZMVWCq3qtJbKNfD7BmCGZ2u6qg_41r7hRA4oxQPRCBOE9Fxq4_-vtQKZy_YFEXuw0aYnvkdv9zoQn9iZhCw8-aY8hv9BBasTprvICAefJaQFGZ-DpOgeUf62yvDsR20k4mKHKaBZSSwKLFQ2QlW0JItw_BpCWOQAXAdeg222YFNKzkeB2pz-tAewy5X_SsOX603VbowTbVSRb0-BjYBfBsi81IhWjS9fs_DTRe8uENhb2g0X8gd0pZQYJeuP2SBkqW7OmHjQbVoJ7I2rERbyQ1OUxRejfL9hzKR_lDdxT7xUztmYmAPgVef6g/file\n",
            "Reusing existing connection to uce0ea463e7d3a8440bf8e57b90c.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 63252113 (60M) [application/zip]\n",
            "Saving to: ‘archive.zip’\n",
            "\n",
            "archive.zip         100%[===================>]  60.32M  65.1MB/s    in 0.9s    \n",
            "\n",
            "2022-05-25 10:08:27 (65.1 MB/s) - ‘archive.zip’ saved [63252113/63252113]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/archive.zip\""
      ],
      "metadata": {
        "id": "gts4fUq2tLM1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"/content/train\" #passing the path with training images\n",
        "test_dir = \"/content/test\"   #passing the path with testing images"
      ],
      "metadata": {
        "id": "a78a6z2NtLKH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = 48 #original size of the image"
      ],
      "metadata": {
        "id": "bh3l-_UqtLH1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Data Augmentation\n",
        "--------------------------\n",
        "rotation_range = rotates the image with the amount of degrees we provide\n",
        "width_shift_range = shifts the image randomly to the right or left along the width of the image\n",
        "height_shift range = shifts image randomly to up or below along the height of the image\n",
        "horizontal_flip = flips the image horizontally\n",
        "rescale = to scale down the pizel values in our image between 0 and 1\n",
        "zoom_range = applies random zoom to our object\n",
        "validation_split = reserves some images to be used for validation purpose\n",
        "\"\"\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(#rotation_range = 180,\n",
        "                                         width_shift_range = 0.1,\n",
        "                                         height_shift_range = 0.1,\n",
        "                                         horizontal_flip = True,\n",
        "                                         rescale = 1./255,\n",
        "                                         #zoom_range = 0.2,\n",
        "                                         validation_split = 0.2\n",
        "                                        )\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                         validation_split = 0.2)"
      ],
      "metadata": {
        "id": "SfGwa9ivtLFH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Applying data augmentation to the images as we read \n",
        "them from their respectivve directories\n",
        "\"\"\"\n",
        "train_generator = train_datagen.flow_from_directory(directory = train_dir,\n",
        "                                                    target_size = (img_size,img_size),\n",
        "                                                    batch_size = 64,\n",
        "                                                    color_mode = \"grayscale\",\n",
        "                                                    class_mode = \"categorical\",\n",
        "                                                    subset = \"training\"\n",
        "                                                   )\n",
        "validation_generator = validation_datagen.flow_from_directory( directory = test_dir,\n",
        "                                                              target_size = (img_size,img_size),\n",
        "                                                              batch_size = 64,\n",
        "                                                              color_mode = \"grayscale\",\n",
        "                                                              class_mode = \"categorical\",\n",
        "                                                              subset = \"validation\"\n",
        "                                                             )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFquzEU6tLCC",
        "outputId": "c1613b6e-6312-4e6e-9288-020eec6dde32"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 22968 images belonging to 7 classes.\n",
            "Found 1432 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model= tf.keras.models.Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(48, 48,1)))\n",
        "model.add(Conv2D(64,(3,3), padding='same', activation='relu' ))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128,(5,5), padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "    \n",
        "model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten()) \n",
        "model.add(Dense(256,activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "    \n",
        "model.add(Dense(512,activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    optimizer = Adam(lr=0.0001), \n",
        "    loss='categorical_crossentropy', \n",
        "    metrics=['accuracy']\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xYzGBgJtK_p",
        "outputId": "5140cddf-70d9-44c3-e3a6-bad41f06a23d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 60\n",
        "batch_size = 64"
      ],
      "metadata": {
        "id": "9x8umWe3tK-C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21VkZ8CstK44",
        "outputId": "5fe5234d-72e4-4b93-c36c-168456d1d1d8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 48, 48, 32)        320       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 48, 48, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 48, 48, 64)       256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 24, 24, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 24, 24, 64)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 24, 24, 128)       204928    \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 24, 24, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 12, 12, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 12, 12, 128)       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 12, 12, 512)       590336    \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 12, 12, 512)      2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 6, 6, 512)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 6, 6, 512)         0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 6, 6, 512)         2359808   \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 6, 6, 512)        2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 3, 3, 512)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 3, 3, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4608)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               1179904   \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 256)              1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               131584    \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 7)                 3591      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,496,903\n",
            "Trainable params: 4,492,935\n",
            "Non-trainable params: 3,968\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x = train_generator,epochs = epochs,validation_data = validation_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZQgwyldtK2V",
        "outputId": "15c61502-986e-4f63-8cd7-70bf510c357e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "359/359 [==============================] - 34s 55ms/step - loss: 9.2726 - accuracy: 0.1915 - val_loss: 8.5757 - val_accuracy: 0.1718\n",
            "Epoch 2/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 8.2067 - accuracy: 0.2282 - val_loss: 7.5332 - val_accuracy: 0.2619\n",
            "Epoch 3/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 7.1885 - accuracy: 0.2486 - val_loss: 6.4861 - val_accuracy: 0.3031\n",
            "Epoch 4/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 6.1934 - accuracy: 0.2757 - val_loss: 5.5523 - val_accuracy: 0.3345\n",
            "Epoch 5/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 5.2891 - accuracy: 0.2991 - val_loss: 4.8651 - val_accuracy: 0.3240\n",
            "Epoch 6/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 4.5325 - accuracy: 0.3299 - val_loss: 4.5507 - val_accuracy: 0.3101\n",
            "Epoch 7/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 3.9118 - accuracy: 0.3504 - val_loss: 3.6370 - val_accuracy: 0.3904\n",
            "Epoch 8/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 3.4196 - accuracy: 0.3788 - val_loss: 3.0629 - val_accuracy: 0.4316\n",
            "Epoch 9/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 3.0187 - accuracy: 0.3976 - val_loss: 2.7734 - val_accuracy: 0.4302\n",
            "Epoch 10/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 2.6990 - accuracy: 0.4290 - val_loss: 2.5348 - val_accuracy: 0.4693\n",
            "Epoch 11/60\n",
            "359/359 [==============================] - 19s 52ms/step - loss: 2.4395 - accuracy: 0.4520 - val_loss: 2.1861 - val_accuracy: 0.5293\n",
            "Epoch 12/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 2.2498 - accuracy: 0.4686 - val_loss: 2.1115 - val_accuracy: 0.5126\n",
            "Epoch 13/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 2.1026 - accuracy: 0.4859 - val_loss: 1.8897 - val_accuracy: 0.5733\n",
            "Epoch 14/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 1.9828 - accuracy: 0.4967 - val_loss: 1.8343 - val_accuracy: 0.5587\n",
            "Epoch 15/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 1.8636 - accuracy: 0.5170 - val_loss: 1.7189 - val_accuracy: 0.5684\n",
            "Epoch 16/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.7854 - accuracy: 0.5255 - val_loss: 1.7113 - val_accuracy: 0.5545\n",
            "Epoch 17/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.7216 - accuracy: 0.5352 - val_loss: 1.6141 - val_accuracy: 0.5789\n",
            "Epoch 18/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.6693 - accuracy: 0.5413 - val_loss: 1.5360 - val_accuracy: 0.5915\n",
            "Epoch 19/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 1.6165 - accuracy: 0.5511 - val_loss: 1.4884 - val_accuracy: 0.6068\n",
            "Epoch 20/60\n",
            "359/359 [==============================] - 19s 52ms/step - loss: 1.5947 - accuracy: 0.5576 - val_loss: 1.5302 - val_accuracy: 0.5705\n",
            "Epoch 21/60\n",
            "359/359 [==============================] - 19s 52ms/step - loss: 1.5512 - accuracy: 0.5653 - val_loss: 1.4524 - val_accuracy: 0.6103\n",
            "Epoch 22/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.5339 - accuracy: 0.5707 - val_loss: 1.4468 - val_accuracy: 0.6180\n",
            "Epoch 23/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 1.5119 - accuracy: 0.5811 - val_loss: 1.4949 - val_accuracy: 0.5992\n",
            "Epoch 24/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.4937 - accuracy: 0.5809 - val_loss: 1.4205 - val_accuracy: 0.6075\n",
            "Epoch 25/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.4779 - accuracy: 0.5870 - val_loss: 1.4954 - val_accuracy: 0.5852\n",
            "Epoch 26/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.4541 - accuracy: 0.5984 - val_loss: 1.3638 - val_accuracy: 0.6369\n",
            "Epoch 27/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.4385 - accuracy: 0.5983 - val_loss: 1.3858 - val_accuracy: 0.6334\n",
            "Epoch 28/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.4340 - accuracy: 0.6019 - val_loss: 1.4535 - val_accuracy: 0.6068\n",
            "Epoch 29/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 1.4297 - accuracy: 0.6041 - val_loss: 1.3544 - val_accuracy: 0.6383\n",
            "Epoch 30/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 1.4145 - accuracy: 0.6131 - val_loss: 1.3935 - val_accuracy: 0.6425\n",
            "Epoch 31/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 1.4077 - accuracy: 0.6148 - val_loss: 1.3565 - val_accuracy: 0.6480\n",
            "Epoch 32/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.4036 - accuracy: 0.6144 - val_loss: 1.4491 - val_accuracy: 0.6180\n",
            "Epoch 33/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.3988 - accuracy: 0.6174 - val_loss: 1.3387 - val_accuracy: 0.6536\n",
            "Epoch 34/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 1.3980 - accuracy: 0.6216 - val_loss: 1.3598 - val_accuracy: 0.6411\n",
            "Epoch 35/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 1.3810 - accuracy: 0.6290 - val_loss: 1.3901 - val_accuracy: 0.6369\n",
            "Epoch 36/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 1.3755 - accuracy: 0.6321 - val_loss: 1.4588 - val_accuracy: 0.6194\n",
            "Epoch 37/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 1.3759 - accuracy: 0.6290 - val_loss: 1.3455 - val_accuracy: 0.6634\n",
            "Epoch 38/60\n",
            "359/359 [==============================] - 19s 52ms/step - loss: 1.3741 - accuracy: 0.6354 - val_loss: 1.3379 - val_accuracy: 0.6494\n",
            "Epoch 39/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 1.3666 - accuracy: 0.6405 - val_loss: 1.3218 - val_accuracy: 0.6613\n",
            "Epoch 40/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 1.3653 - accuracy: 0.6363 - val_loss: 1.3559 - val_accuracy: 0.6606\n",
            "Epoch 41/60\n",
            "359/359 [==============================] - 19s 52ms/step - loss: 1.3508 - accuracy: 0.6426 - val_loss: 1.3486 - val_accuracy: 0.6627\n",
            "Epoch 42/60\n",
            "359/359 [==============================] - 19s 52ms/step - loss: 1.3534 - accuracy: 0.6422 - val_loss: 1.4130 - val_accuracy: 0.6334\n",
            "Epoch 43/60\n",
            "359/359 [==============================] - 19s 52ms/step - loss: 1.3565 - accuracy: 0.6453 - val_loss: 1.3306 - val_accuracy: 0.6627\n",
            "Epoch 44/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 1.3507 - accuracy: 0.6488 - val_loss: 1.3973 - val_accuracy: 0.6543\n",
            "Epoch 45/60\n",
            "359/359 [==============================] - 19s 53ms/step - loss: 1.3544 - accuracy: 0.6473 - val_loss: 1.3440 - val_accuracy: 0.6599\n",
            "Epoch 46/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.3530 - accuracy: 0.6498 - val_loss: 1.3288 - val_accuracy: 0.6795\n",
            "Epoch 47/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 1.3443 - accuracy: 0.6529 - val_loss: 1.3798 - val_accuracy: 0.6480\n",
            "Epoch 48/60\n",
            "359/359 [==============================] - 18s 51ms/step - loss: 1.3541 - accuracy: 0.6489 - val_loss: 1.4001 - val_accuracy: 0.6446\n",
            "Epoch 49/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.3492 - accuracy: 0.6554 - val_loss: 1.3459 - val_accuracy: 0.6725\n",
            "Epoch 50/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.3385 - accuracy: 0.6577 - val_loss: 1.4014 - val_accuracy: 0.6529\n",
            "Epoch 51/60\n",
            "359/359 [==============================] - 19s 52ms/step - loss: 1.3354 - accuracy: 0.6594 - val_loss: 1.3504 - val_accuracy: 0.6690\n",
            "Epoch 52/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.3330 - accuracy: 0.6624 - val_loss: 1.3702 - val_accuracy: 0.6676\n",
            "Epoch 53/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.3321 - accuracy: 0.6599 - val_loss: 1.3642 - val_accuracy: 0.6550\n",
            "Epoch 54/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.3315 - accuracy: 0.6655 - val_loss: 1.3970 - val_accuracy: 0.6508\n",
            "Epoch 55/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.3277 - accuracy: 0.6689 - val_loss: 1.3481 - val_accuracy: 0.6739\n",
            "Epoch 56/60\n",
            "359/359 [==============================] - 18s 49ms/step - loss: 1.3363 - accuracy: 0.6641 - val_loss: 1.3565 - val_accuracy: 0.6669\n",
            "Epoch 57/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.3260 - accuracy: 0.6703 - val_loss: 1.3392 - val_accuracy: 0.6718\n",
            "Epoch 58/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.3247 - accuracy: 0.6687 - val_loss: 1.3816 - val_accuracy: 0.6585\n",
            "Epoch 59/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.3219 - accuracy: 0.6707 - val_loss: 1.3771 - val_accuracy: 0.6627\n",
            "Epoch 60/60\n",
            "359/359 [==============================] - 18s 50ms/step - loss: 1.3222 - accuracy: 0.6701 - val_loss: 1.3936 - val_accuracy: 0.6676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "23GTlG5rtKz6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gH20SY8x4qNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Egjiaku_4s_q"
      }
    }
  ]
}